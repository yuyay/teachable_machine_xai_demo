import streamlit as st
import numpy as np
import cv2
import tensorflow as tf
from PIL import Image, ImageOps
import matplotlib.pyplot as plt
from typing import Tuple
import zipfile
import os
import tempfile


class TeachableMachineModel:
    """Wrapper for Teachable Machine TensorFlow models"""
    
    def __init__(self, model_path: str, labels_path: str = None):
        self.model = tf.keras.models.load_model(model_path)
        self.class_names = self._load_class_names(labels_path) if labels_path else None
        self._analyze_model_structure()
    
    def _load_class_names(self, labels_path: str) -> list:
        """Load class names from labels.txt file"""
        try:
            with open(labels_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            class_names = []
            for line in lines:
                line = line.strip()
                if line:
                    # Format: "0 iPhone" -> extract "iPhone"
                    parts = line.split(' ', 1)
                    if len(parts) >= 2:
                        class_names.append(parts[1])
                    else:
                        class_names.append(parts[0])
            
            return class_names
        except Exception as e:
            print(f"Error loading labels: {e}")
            return None
    
    def _analyze_model_structure(self):
        """Analyze and print the Teachable Machine model structure"""
        print("=== Teachable Machine Model Analysis ===")
        print(f"Model type: {type(self.model)}")
        print(f"Input shape: {self.model.input_shape}")
        print(f"Output shape: {self.model.output_shape}")
        print(f"Total parameters: {self.model.count_params():,}")
        
        print("\nModel Summary:")
        self.model.summary()
        
        print("\nDetailed Layer Structure:")
        for i, layer in enumerate(self.model.layers):
            print(f"Layer {i}: {layer.name}")
            print(f"  Type: {layer.__class__.__name__}")
            print(f"  Output shape: {layer.output_shape}")
            
            # If it's a Sequential/Functional model, show sublayers
            if hasattr(layer, 'layers') and len(layer.layers) > 0:
                print(f"  Sublayers ({len(layer.layers)}):")
                for j, sublayer in enumerate(layer.layers):
                    print(f"    {j}: {sublayer.name} ({sublayer.__class__.__name__}) - {sublayer.output_shape}")
                    
                    # Check for convolutional layers
                    if 'conv' in sublayer.__class__.__name__.lower():
                        print(f"      -> Convolutional layer found!")
            print()
        
        print("=" * 45)
        
    def predict(self, image: np.ndarray) -> Tuple[np.ndarray, int]:
        """Predict class probabilities for an image using Teachable Machine preprocessing"""
        # Convert BGR to RGB if needed
        if len(image.shape) == 3 and image.shape[2] == 3:
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image_rgb = image
        
        # Resize and center crop image using PIL for consistency with Teachable Machine
        pil_image = Image.fromarray(image_rgb.astype(np.uint8))
        # Use ImageOps.fit for center cropping (same as Teachable Machine)
        pil_image = ImageOps.fit(pil_image, (224, 224), Image.Resampling.LANCZOS)
        
        # Convert back to numpy array
        image_array = np.asarray(pil_image)
        
        # Apply Teachable Machine normalization: (pixel / 127.5) - 1
        normalized_image = (image_array.astype(np.float32) / 127.5) - 1
        
        # Add batch dimension
        image_batch = np.expand_dims(normalized_image, axis=0)
        
        predictions = self.model.predict(image_batch, verbose=0)
        predicted_class = np.argmax(predictions[0])
        
        return predictions[0], predicted_class

def extract_teachable_machine_files(zip_file) -> Tuple[str, str]:
    """Extract keras_model.h5 and labels.txt from uploaded zip file"""
    import shutil
    
    # Create temp directory in current working directory for Streamlit Cloud compatibility
    temp_dir = "temp_extract"
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir, exist_ok=True)
    
    try:
        # Save uploaded file to temporary location
        zip_path = os.path.join(temp_dir, "uploaded.zip")
        with open(zip_path, "wb") as f:
            f.write(zip_file.getbuffer())
        
        # Extract zip file
        extract_dir = os.path.join(temp_dir, "extracted")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        
        # Find keras_model.h5 and labels.txt
        model_path = None
        labels_path = None
        
        for root, dirs, files in os.walk(extract_dir):
            for file in files:
                if file == "keras_model.h5":
                    # Copy to current directory
                    model_path = "temp_keras_model.h5"
                    if os.path.exists(model_path):
                        os.remove(model_path)
                    shutil.copy(os.path.join(root, file), model_path)
                elif file == "labels.txt":
                    # Copy to current directory
                    labels_path = "temp_labels.txt"
                    if os.path.exists(labels_path):
                        os.remove(labels_path)
                    shutil.copy(os.path.join(root, file), labels_path)
        
        if not model_path:
            raise FileNotFoundError("keras_model.h5 not found in zip file")
        
        return model_path, labels_path
    
    finally:
        # Clean up temp directory
        if os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
            except:
                pass  # Ignore cleanup errors

class TensorFlowXAIVisualizer:
    """TensorFlow/Keras implementation of XAI visualization using Integrated Gradients"""
    
    def __init__(self, model, layer_name: str = None):
        self.model = model
        self.layer_name = layer_name
        self.target_conv_layer = None
        self._find_target_conv_layer()
        
    def _find_target_conv_layer(self):
        """Find the best convolutional layer for GradCAM visualization"""
        conv_layers = []
        
        print("=== Finding Target Conv Layer ===")
        
        # Search deeper into nested models (like MobileNet in Teachable Machine)
        def search_layers_recursively(layers, path=""):
            for i, layer in enumerate(layers):
                current_path = f"{path}[{i}]" if path else f"[{i}]"
                layer_type = layer.__class__.__name__.lower()
                
                # Check if this layer itself is a conv layer
                if any(keyword in layer_type for keyword in ['conv', 'separable', 'depthwise']):
                    conv_layers.append((layer, current_path, layer.name))
                    print(f"Found conv layer: {layer.name} at {current_path}")
                
                # Recursively search in sublayers
                if hasattr(layer, 'layers') and len(layer.layers) > 0:
                    search_layers_recursively(layer.layers, current_path)
        
        # Start recursive search
        search_layers_recursively(self.model.layers)
        
        if conv_layers:
            # Use the last convolutional layer found
            self.target_conv_layer = conv_layers[-1]
            print(f"Using target layer: {self.target_conv_layer[2]} at {self.target_conv_layer[1]}")
        else:
            # Fallback: try to use the feature extractor output
            print("No conv layers found, trying feature extractor...")
            # For Teachable Machine, use the output of the first sequential (feature extractor)
            if len(self.model.layers) > 0 and hasattr(self.model.layers[0], 'layers'):
                feature_extractor = self.model.layers[0].layers[0]  # model1
                self.target_conv_layer = (feature_extractor, "[0][0]", "feature_extractor")
                print(f"Using feature extractor: {feature_extractor.name}")
            else:
                self.target_conv_layer = None
                print("No suitable layers found!")
        
        print("=" * 35)
    
    def generate_explanation(self, image: np.ndarray, class_idx: int) -> Tuple[np.ndarray, np.ndarray]:
        """Generate XAI explanation visualization"""
        return self._generate_explanation(image, class_idx)
    
    def _generate_explanation(self, image: np.ndarray, class_idx: int) -> Tuple[np.ndarray, np.ndarray]:
        """Generate XAI explanation using Integrated Gradients for complex models"""
        img_array = self._preprocess_image(image)
        
        # Try to use a simpler approach for nested/transfer learning models
        try:
            return self._generate_integrated_gradients(img_array, image, class_idx)
        except Exception as e:
            print(f"Integrated gradients failed: {e}")
            # Fallback to guided backpropagation
            return self._generate_guided_backprop(img_array, image, class_idx)
    
    def _generate_integrated_gradients(self, img_array: tf.Tensor, original_image: np.ndarray, class_idx: int) -> Tuple[np.ndarray, np.ndarray]:
        """Generate visualization using Integrated Gradients method"""
        baseline = tf.zeros_like(img_array)
        m_steps = 50
        
        # Generate path from baseline to input
        alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1)
        
        def interpolate_images(baseline, image, alphas):
            alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]
            baseline_x = tf.expand_dims(baseline, axis=0)
            input_x = tf.expand_dims(image, axis=0)
            delta = input_x - baseline_x
            images = baseline_x + alphas_x * delta
            return images
        
        # Generate interpolated inputs
        interpolated_images = interpolate_images(baseline[0], img_array[0], alphas)
        
        # Compute gradients
        with tf.GradientTape() as tape:
            tape.watch(interpolated_images)
            predictions = self.model(interpolated_images)
            class_outputs = predictions[:, class_idx]
        
        grads = tape.gradient(class_outputs, interpolated_images)
        
        # Average gradients and compute integrated gradients
        avg_grads = tf.reduce_mean(grads, axis=0)
        integrated_grads = (img_array[0] - baseline[0]) * avg_grads
        
        # Convert to importance map
        importance = tf.reduce_mean(tf.abs(integrated_grads), axis=-1)
        
        # Normalize
        importance = (importance - tf.reduce_min(importance)) / (tf.reduce_max(importance) - tf.reduce_min(importance) + 1e-8)
        heatmap = importance.numpy()
        
        # Apply smoothing
        heatmap = self._smooth_heatmap(heatmap, sigma=2.0)
        
        return self._create_visualization(original_image, heatmap)
    
    def _generate_guided_backprop(self, img_array: tf.Tensor, original_image: np.ndarray, class_idx: int) -> Tuple[np.ndarray, np.ndarray]:
        """Generate visualization using guided backpropagation"""
        with tf.GradientTape() as tape:
            tape.watch(img_array)
            predictions = self.model(img_array)
            class_output = predictions[0][class_idx]
        
        # Compute gradients with respect to input
        grads = tape.gradient(class_output, img_array)
        
        if grads is None:
            raise ValueError("Could not compute gradients")
        
        # Convert gradients to importance map
        grads = tf.abs(grads[0])  # Take absolute value and remove batch dimension
        importance = tf.reduce_mean(grads, axis=-1)  # Average over color channels
        
        # Normalize
        importance = (importance - tf.reduce_min(importance)) / (tf.reduce_max(importance) - tf.reduce_min(importance) + 1e-8)
        heatmap = importance.numpy()
        
        # Apply smoothing
        heatmap = self._smooth_heatmap(heatmap, sigma=2.0)
        
        return self._create_visualization(original_image, heatmap)
    
    def _smooth_heatmap(self, heatmap: np.ndarray, sigma: float = 1.0) -> np.ndarray:
        """Apply Gaussian smoothing to heatmap for better visualization"""
        try:
            from scipy.ndimage import gaussian_filter
            # Apply Gaussian filter for smoothing
            smoothed = gaussian_filter(heatmap, sigma=sigma)
            return smoothed
        except ImportError:
            # Fallback: use OpenCV Gaussian blur if scipy is not available
            kernel_size = max(3, int(2 * np.ceil(2 * sigma) + 1))  # Ensure odd kernel size >= 3
            if kernel_size % 2 == 0:
                kernel_size += 1
            smoothed = cv2.GaussianBlur(heatmap, (kernel_size, kernel_size), sigma)
            return smoothed
    
    def _create_visualization(self, image: np.ndarray, heatmap: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Create visualization using 224x224 center-cropped image and heatmap"""
        # Convert original image to RGB
        if image.shape[2] == 3:
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image_rgb = image
        
        # Apply the same center cropping as used for model input
        pil_image = Image.fromarray(image_rgb.astype(np.uint8))
        pil_image_cropped = ImageOps.fit(pil_image, (224, 224), Image.Resampling.LANCZOS)
        cropped_image = np.asarray(pil_image_cropped)
        
        # Use the center-cropped 224x224 image
        rgb_img = cropped_image.astype(np.float32) / 255.0
        
        # Create smooth heatmap overlay with better color mapping
        # Apply smoothing to the heatmap (already 224x224)
        kernel_size = 11  # Fixed kernel size for 224x224
        heatmap_smooth = cv2.GaussianBlur(heatmap, (kernel_size, kernel_size), 2.0)
        
        # Enhance contrast for better visibility
        heatmap_enhanced = np.power(heatmap_smooth, 0.7)  # Gamma correction for better contrast
        heatmap_enhanced = np.clip(heatmap_enhanced, 0, 1)
        
        # Use JET colormap consistently (red = high importance, blue = low importance)
        heatmap_colored_bgr = cv2.applyColorMap(np.uint8(255 * heatmap_enhanced), cv2.COLORMAP_JET)
        # Convert BGR to RGB for proper overlay
        heatmap_colored = cv2.cvtColor(heatmap_colored_bgr, cv2.COLOR_BGR2RGB)
        heatmap_colored = heatmap_colored.astype(np.float32) / 255.0
        
        # Overlay the heatmap on the original image (both in RGB format)
        # Use alpha blending based on heatmap intensity for better visibility
        alpha = heatmap_enhanced[..., np.newaxis] * 0.6 + 0.2  # Dynamic alpha based on importance
        overlay = (1 - alpha) * rgb_img + alpha * heatmap_colored
        overlay = np.clip(overlay, 0, 1)
        
        # Convert back to BGR for consistency with OpenCV
        overlay_bgr = cv2.cvtColor((overlay * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)
        
        # Create heatmap for separate display (using the original 224x224 heatmap)
        heatmap_display = np.uint8(255 * heatmap)
        
        return overlay_bgr, heatmap_display
    
    def _preprocess_image(self, image: np.ndarray) -> tf.Tensor:
        """Preprocess image using Teachable Machine format"""
        # Convert BGR to RGB if needed
        if len(image.shape) == 3 and image.shape[2] == 3:
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image_rgb = image
        
        # Resize and center crop using PIL for consistency with Teachable Machine
        pil_image = Image.fromarray(image_rgb.astype(np.uint8))
        # Use ImageOps.fit for center cropping (same as Teachable Machine)
        pil_image = ImageOps.fit(pil_image, (224, 224), Image.Resampling.LANCZOS)
        
        # Convert back to numpy array
        image_array = np.asarray(pil_image)
        
        # Apply Teachable Machine normalization: (pixel / 127.5) - 1
        normalized_image = (image_array.astype(np.float32) / 127.5) - 1
        
        # Add batch dimension
        img_array = np.expand_dims(normalized_image, axis=0)
        
        return tf.convert_to_tensor(img_array)

def main():
    # Set page config for better mobile compatibility
    st.set_page_config(
        page_title="XAI Demo - Teachable Machine + GradCAM", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ¤– XAI Demo: Teachable Machine + Integrated Gradients")
    st.markdown("Teachable Machineã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦Webã‚«ãƒ¡ãƒ©ç”»åƒã‚’åˆ†é¡ã—ã€Integrated Gradientsã§é‡è¦é ˜åŸŸã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")
    
    # Privacy notice
    with st.expander("ğŸ”’ ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã«ã¤ã„ã¦"):
        st.markdown("""
        **ã“ã®ã‚¢ãƒ—ãƒªã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼:**
        - ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã¯ã‚µãƒ¼ãƒãƒ¼ã«ä¿å­˜ã•ã‚Œã¾ã›ã‚“
        - ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸€æ™‚çš„ã«ã®ã¿å‡¦ç†ã•ã‚Œã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†å¾Œã«å‰Šé™¤ã•ã‚Œã¾ã™
        - æ’®å½±ã•ã‚ŒãŸç”»åƒã¯åˆ†æå¾Œã«ãƒ¡ãƒ¢ãƒªã‹ã‚‰å‰Šé™¤ã•ã‚Œã¾ã™
        - å€‹äººæƒ…å ±ã¯åé›†ã•ã‚Œã¾ã›ã‚“
        """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ¢ãƒ‡ãƒ«è¨­å®š
    st.sidebar.header("ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    
    # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_zip = st.sidebar.file_uploader(
        "Teachable Machineãƒ¢ãƒ‡ãƒ«(.zip)ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['zip'],
        help="Google Teachable Machineã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸzipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆkeras_model.h5ã¨labels.txtã‚’å«ã‚€ï¼‰"
    )
    
    if uploaded_zip is not None:
        try:
            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
            with st.spinner("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºä¸­..."):
                model_path, labels_path = extract_teachable_machine_files(uploaded_zip)
            
            # ãƒ¢ãƒ‡ãƒ«ã¨XAIå¯è¦–åŒ–ã®åˆæœŸåŒ–
            tm_model = TeachableMachineModel(model_path, labels_path)
            xai_visualizer = TensorFlowXAIVisualizer(tm_model.model)
            
            st.success("ãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸï¼")
            
            # Display model info
            if tm_model.class_names:
                st.info(f"ã‚¯ãƒ©ã‚¹æ•°: {len(tm_model.class_names)} ã‚¯ãƒ©ã‚¹")
                with st.expander("ã‚¯ãƒ©ã‚¹ä¸€è¦§"):
                    for i, name in enumerate(tm_model.class_names):
                        st.write(f"{i}: {name}")
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            col1, col2 = st.columns(2)
            
            with col1:
                st.header("ğŸ“· Webã‚«ãƒ¡ãƒ©")
                
                # Webã‚«ãƒ¡ãƒ©ã‹ã‚‰ã®ç”»åƒå–å¾—
                camera_input = st.camera_input("å†™çœŸã‚’æ’®å½±ã—ã¦ãã ã•ã„")
                
                if camera_input is not None:
                    # ç”»åƒã®èª­ã¿è¾¼ã¿ã¨å¤‰æ›
                    image = Image.open(camera_input)
                    image_array = np.array(image)
                    
                    # RGB to BGR (OpenCVç”¨)
                    if len(image_array.shape) == 3:
                        image_bgr = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
                    else:
                        image_bgr = image_array
                    
                    # äºˆæ¸¬å®Ÿè¡Œ
                    predictions, predicted_class = tm_model.predict(image_bgr)
                    
                    # çµæœè¡¨ç¤º
                    st.subheader("ğŸ¯ äºˆæ¸¬çµæœ")
                    
                    if tm_model.class_names and len(tm_model.class_names) > predicted_class:
                        st.write(f"**äºˆæ¸¬ã‚¯ãƒ©ã‚¹:** {tm_model.class_names[predicted_class]}")
                    else:
                        st.write(f"**äºˆæ¸¬ã‚¯ãƒ©ã‚¹:** Class {predicted_class}")
                    
                    st.write(f"**ä¿¡é ¼åº¦:** {predictions[predicted_class]:.2%}")
                    
                    # å…¨ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡è¡¨ç¤º
                    st.subheader("ğŸ“Š å…¨ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡")
                    for i, prob in enumerate(predictions):
                        label = tm_model.class_names[i] if tm_model.class_names and i < len(tm_model.class_names) else f"Class {i}"
                        st.write(f"{label}: {prob:.2%}")
            
            with col2:
                st.header("ğŸ” XAIå¯è¦–åŒ–")
                
                if camera_input is not None:
                    # XAIå¯è¦–åŒ–ã®ç”Ÿæˆ
                    with st.spinner("Integrated Gradientsã‚’ç”Ÿæˆä¸­..."):
                        try:
                            overlay, heatmap = xai_visualizer.generate_explanation(image_bgr, predicted_class)
                            
                            # BGR to RGB (è¡¨ç¤ºç”¨)
                            overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)
                            
                            st.image(overlay_rgb, caption="Integrated Gradientså¯è¦–åŒ–çµæœ", use_column_width=True)
                            
                            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ã¿ã‚‚è¡¨ç¤º
                            st.subheader("ğŸŒ¡ï¸ é‡è¦åº¦ãƒãƒƒãƒ—")
                            fig, ax = plt.subplots(figsize=(6, 6))
                            ax.imshow(heatmap, cmap='jet')
                            ax.axis('off')
                            ax.set_title('Integrated Gradients Importance Map')
                            
                            st.pyplot(fig)
                            plt.close()
                            
                        except Exception as e:
                            st.error(f"XAIå¯è¦–åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                            st.info("ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å‡¦ç†ã§å•é¡ŒãŒç™ºç”Ÿã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                        
                        # èª¬æ˜æ–‡
                        st.info("**è‰²ã®æ„å‘³:** èµ¤ã„é ˜åŸŸã»ã©ãƒ¢ãƒ‡ãƒ«ãŒåˆ†é¡ã®åˆ¤æ–­ã«é‡è¦è¦–ã—ã¦ã„ã‚‹éƒ¨åˆ†ã€é’ã„é ˜åŸŸã¯é‡è¦åº¦ãŒä½ã„éƒ¨åˆ†ã§ã™ã€‚")
                
                else:
                    st.info("Webã‚«ãƒ¡ãƒ©ã§å†™çœŸã‚’æ’®å½±ã™ã‚‹ã¨ã€XAIå¯è¦–åŒ–çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        
        except Exception as e:
            st.error(f"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            st.info("Teachable Machineã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸzipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚keras_model.h5ã¨labels.txtãŒå«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
        
        finally:
            # Clean up temporary files for security
            for temp_file in ["temp_keras_model.h5", "temp_labels.txt"]:
                if os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                    except:
                        pass
    
    else:
        st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Teachable Machineãƒ¢ãƒ‡ãƒ«ï¼ˆzipï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        
        # ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜
        st.markdown("""
        ## ğŸ“ ä½¿ç”¨æ–¹æ³•
        
        1. **ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™**
           - [Google Teachable Machine](https://teachablemachine.withgoogle.com/)ã§ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
           - ãƒ¢ãƒ‡ãƒ«ã‚’TensorFlowå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆzipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
           
        2. **ã‚¢ãƒ—ãƒªã®è¨­å®š**
           - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
           - keras_model.h5ã¨labels.txtãŒè‡ªå‹•ã§èª­ã¿è¾¼ã¾ã‚Œã¾ã™
           
        3. **ç”»åƒåˆ†é¡ã¨XAI**
           - Webã‚«ãƒ¡ãƒ©ã§å†™çœŸã‚’æ’®å½±
           - è‡ªå‹•ã§åˆ†é¡çµæœã¨Integrated Gradientså¯è¦–åŒ–ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
        
        ## ğŸ¯ XAIï¼ˆèª¬æ˜å¯èƒ½AIï¼‰ã«ã¤ã„ã¦
        
        ã“ã®ãƒ‡ãƒ¢ã§ã¯**Integrated Gradients**ã‚’ä½¿ç”¨ã—ã¦ã€AIãŒã©ã®éƒ¨åˆ†ã‚’é‡è¦è¦–ã—ã¦åˆ¤æ–­ã—ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã¦ã„ã¾ã™ã€‚
        - Teachable Machineã®TensorFlowãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ä½¿ç”¨
        - è»¢ç§»å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«æœ€é©åŒ–ã•ã‚ŒãŸå¯è¦–åŒ–æ‰‹æ³•
        - èµ¤ã„é ˜åŸŸã»ã©ã€ãƒ¢ãƒ‡ãƒ«ãŒåˆ†é¡ã®æ±ºå®šã«é‡è¦ã¨åˆ¤æ–­ã—ãŸéƒ¨åˆ†ã§ã™ã€‚
        """)

if __name__ == "__main__":
    main()